{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db42f7d4",
   "metadata": {},
   "source": [
    "## BPE TOKENIZER - COMPLETE IMPLEMENTATION\n",
    "\n",
    "A custom implementation of Byte Pair Encoding (BPE) tokenization algorithm.\n",
    "This is the same algorithm used in modern language models like GPT.\n",
    "\n",
    "Author: √Ångel Morales Romero\n",
    "Date: November 2024\n",
    "License: MIT\n",
    "\n",
    "Requirements:\n",
    "    - datasets (HuggingFace)\n",
    "    - tqdm\n",
    "\n",
    "Installation:\n",
    "    pip install datasets tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20eb27",
   "metadata": {},
   "source": [
    "### SECTION 1: IMPORTS\n",
    "\n",
    "We only need minimal dependencies:\n",
    "- datasets: To load text corpora from HuggingFace\n",
    "- collections.Counter: To count character pair frequencies\n",
    "- tqdm: For progress bars during training\n",
    "- os: For file system operations\n",
    "- re: For regex-based pretokenization (separating punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3dbc77bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593745f",
   "metadata": {},
   "source": [
    "### SECTION 2: DATASET CONFIGURATION\n",
    "\n",
    "Dataset Configuration Dictionary\n",
    "\n",
    "Define your available datasets here. Each dataset needs:\n",
    "- dataset_name: Local identifier\n",
    "- hf_dataset_name: HuggingFace dataset identifier\n",
    "- text_column: Column name containing the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dea38ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"stories\": {\n",
    "        \"dataset_name\": \"AI_Storyteller_Dataset\",\n",
    "        \"hf_dataset_name\": \"jaydenccc/AI_Storyteller_Dataset\",\n",
    "        \"text_column\": \"short_story\",\n",
    "    },\n",
    "    \"harry_potter\": {\n",
    "        \"dataset_name\": \"HarryPotter_books_1to7\",\n",
    "        \"hf_dataset_name\": \"WutYee/HarryPotter_books_1to7\",\n",
    "        \"text_column\": \"text\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8674b73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 3: LOAD DATASET FROM HUGGINGFACE\n",
    "\n",
    "Loading the Dataset\n",
    "\n",
    "This section downloads and loads a dataset from HuggingFace.\n",
    "You can modify the dataset selection by changing the key below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97a5d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_dataset(dataset_key=\"stories\"):\n",
    "    \"\"\"\n",
    "    Load a dataset from HuggingFace.\n",
    "    \n",
    "    Args:\n",
    "        dataset_key (str): Key from DATASETS dictionary\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (dataset, dataset_name, text_column)\n",
    "    \"\"\"\n",
    "    dataset_config = DATASETS[dataset_key]\n",
    "    \n",
    "    dataset_name = dataset_config[\"dataset_name\"]\n",
    "    hf_dataset_name = dataset_config[\"hf_dataset_name\"]\n",
    "    text_column = dataset_config[\"text_column\"]\n",
    "    \n",
    "    print(f\"Loading dataset: {hf_dataset_name}\")\n",
    "    dataset = load_dataset(hf_dataset_name)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Splits: {list(dataset.keys())}\")\n",
    "    print(f\"Training examples: {len(dataset['train'])}\")\n",
    "    \n",
    "    # Show a few examples\n",
    "    print(f\"\\nFirst 3 examples:\")\n",
    "    for i, example in enumerate(dataset[\"train\"][:3][text_column]):\n",
    "        print(f\"  [{i+1}] {example[:50]}...\")\n",
    "    \n",
    "    return dataset, dataset_name, text_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670de94",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 4: SAVE DATASET TO TEXT FILE\n",
    "\n",
    "Save Dataset to File\n",
    "\n",
    "Convert the HuggingFace dataset to a plain text file for easier processing.\n",
    "This also allows you to inspect the data manually if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03a5d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_to_file(dataset, dataset_name, text_column, \n",
    "                         output_dir=\"./data\", fraction=1.0):\n",
    "    \"\"\"\n",
    "    Save dataset to a text file.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset object\n",
    "        dataset_name (str): Name for output file\n",
    "        text_column (str): Column containing text\n",
    "        output_dir (str): Directory to save files\n",
    "        fraction (float): Fraction of data to use (0.0 to 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (full_text, output_path)\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Combine all text\n",
    "    print(\"\\nCombining text from all examples...\")\n",
    "    text = \"\"\n",
    "    for item in tqdm(dataset[\"train\"], desc=\"Processing\"):\n",
    "        text += item[text_column] + \"\\n\"\n",
    "    \n",
    "    # Apply fraction if specified\n",
    "    if fraction < 1.0:\n",
    "        text = text[:int(len(text) * fraction)]\n",
    "        output_path = f\"{output_dir}/{dataset_name}_small.txt\"\n",
    "        print(f\"\\nUsing {fraction*100:.0f}% of the data\")\n",
    "    else:\n",
    "        output_path = f\"{output_dir}/{dataset_name}.txt\"\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    # Statistics\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Text saved to: {output_path}\")\n",
    "    print(f\"   Characters: {char_count:,} ({char_count/1_000_000:.2f}M)\")\n",
    "    print(f\"   Words: {word_count:,}\")\n",
    "    print(f\"   Lines: {text.count(chr(10)):,}\")\n",
    "    \n",
    "    return text, output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94321c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 5: PREPARE TEXT FOR BPE TRAINING\n",
    "\n",
    "Text Preprocessing for BPE\n",
    "\n",
    "Key concepts:\n",
    "1. Normalize text (convert to lowercase)\n",
    "2. Split into words\n",
    "3. Add end-of-word marker '</w>' to preserve word boundaries\n",
    "4. Convert each word to a list of characters\n",
    "\n",
    "Example:\n",
    "    \"Hello World\" ‚Üí ['h', 'e', 'l', 'l', 'o', '</w>'] ['w', 'o', 'r', 'l', 'd', '</w>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d495f6",
   "metadata": {},
   "source": [
    "### SECTION 4.5: PRETOKENIZATION\n",
    "\n",
    "Pretokenization Function\n",
    "\n",
    "**What is Pretokenization?**\n",
    "\n",
    "Pretokenization is the FIRST step before BPE. It splits text into basic units (pretokens) that should never be merged across. This is crucial for:\n",
    "\n",
    "1. **Separating punctuation**: \"Hello, world!\" ‚Üí [\"Hello\", \",\", \"world\", \"!\"]\n",
    "2. **Handling contractions**: \"don't\" ‚Üí [\"don\", \"'\", \"t\"]\n",
    "3. **Preserving numbers**: \"2024\" stays as one pretoken\n",
    "4. **Splitting on whitespace**: Multiple spaces are normalized\n",
    "\n",
    "**Why is it important?**\n",
    "\n",
    "Without pretokenization:\n",
    "- \"hello,world\" would learn \",w\" as a common token (wrong!)\n",
    "- Punctuation would be glued to words\n",
    "- Poor generalization to new text\n",
    "\n",
    "With pretokenization:\n",
    "- Each punctuation mark is its own token\n",
    "- Words and punctuation are independent\n",
    "- Better compression and generalization\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input:  \"Hello, world! It's 2024.\"\n",
    "Output: [\"hello\", \",\", \"world\", \"!\", \"it\", \"'\", \"s\", \"2024\", \".\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3737b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(text):\n",
    "    \"\"\"\n",
    "    Pretokenize text by separating punctuation and splitting on whitespace.\n",
    "    \n",
    "    This is the FIRST step before BPE. It ensures that:\n",
    "    - Punctuation is separated from words\n",
    "    - Contractions are split (e.g., \"don't\" ‚Üí \"don\", \"'\", \"t\")\n",
    "    - Numbers are preserved as single tokens\n",
    "    - Multiple spaces are handled correctly\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text\n",
    "        \n",
    "    Returns:\n",
    "        list: List of pretokens (words and punctuation)\n",
    "        \n",
    "    Example:\n",
    "        >>> pretokenize(\"Hello, world! It's 2024.\")\n",
    "        ['Hello', ',', 'world', '!', 'It', \"'\", 's', '2024', '.']\n",
    "    \"\"\"\n",
    "    # Pattern explanation:\n",
    "    # \\w+        : Match word characters (letters, digits, underscore)\n",
    "    # '[a-z]*    : Match contractions like 's, 't, 're\n",
    "    # \\d+        : Match numbers\n",
    "    # [^\\s\\w]+   : Match punctuation (anything that's not space or word char)\n",
    "    # \\S         : Match any remaining non-whitespace\n",
    "    \n",
    "    pattern = r\"\\w+(?:'[a-z]*)?|\\d+|[^\\s\\w]+|\\S\"\n",
    "    \n",
    "    # Find all matches and convert to lowercase\n",
    "    tokens = re.findall(pattern, text.lower())\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3f2dc",
   "metadata": {},
   "source": [
    "### TEST PRETOKENIZATION\n",
    "\n",
    "Quick test to verify pretokenization is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e81b4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETOKENIZATION TEST\n",
      "================================================================================\n",
      "\n",
      "Input: Hello, world! How are you?\n",
      "Tokens: ['hello', ',', 'world', '!', 'how', 'are', 'you', '?']\n",
      "Count: 8 tokens\n",
      "\n",
      "Input: It's a beautiful day, isn't it?\n",
      "Tokens: [\"it's\", 'a', 'beautiful', 'day', ',', \"isn't\", 'it', '?']\n",
      "Count: 8 tokens\n",
      "\n",
      "Input: The price is $19.99 (on sale).\n",
      "Tokens: ['the', 'price', 'is', '$', '19', '.', '99', '(', 'on', 'sale', ').']\n",
      "Count: 11 tokens\n",
      "\n",
      "Input: She said: 'Don't worry!' and left.\n",
      "Tokens: ['she', 'said', ':', \"'\", \"don't\", 'worry', \"!'\", 'and', 'left', '.']\n",
      "Count: 10 tokens\n",
      "\n",
      "Input: Year 2024... what's next?\n",
      "Tokens: ['year', '2024', '...', \"what's\", 'next', '?']\n",
      "Count: 6 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test pretokenization with various examples\n",
    "test_sentences = [\n",
    "    \"Hello, world! How are you?\",\n",
    "    \"It's a beautiful day, isn't it?\",\n",
    "    \"The price is $19.99 (on sale).\",\n",
    "    \"She said: 'Don't worry!' and left.\",\n",
    "    \"Year 2024... what's next?\"\n",
    "]\n",
    "\n",
    "print(\"PRETOKENIZATION TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokens = pretokenize(sentence)\n",
    "    print(f\"\\nInput: {sentence}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Count: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6049d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_bpe(text):\n",
    "    \"\"\"\n",
    "    Prepare text for BPE training with proper pretokenization.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (word_tokens, vocab, words)\n",
    "            - word_tokens: List of words as character lists\n",
    "            - vocab: Initial character vocabulary\n",
    "            - words: List of words with </w> markers\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPARING TEXT FOR BPE TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: PRETOKENIZE - Split into words and punctuation\n",
    "    print(\"\\n1Ô∏è‚É£  Pretokenizing text...\")\n",
    "    words_raw = pretokenize(text)\n",
    "    print(f\"   Total pretokens: {len(words_raw):,}\")\n",
    "    print(f\"   Example: {words_raw[:10]}\")\n",
    "    \n",
    "    # Step 2: Add end-of-word markers\n",
    "    words = [word + '</w>' for word in words_raw]\n",
    "    print(f\"\\n2Ô∏è‚É£  Added </w> markers\")\n",
    "    print(f\"   Example: {words[:10]}\")\n",
    "    \n",
    "    # Step 3: Extract character vocabulary\n",
    "    vocab_chars = set()\n",
    "    for word in words:\n",
    "        vocab_chars.update(list(word))\n",
    "    \n",
    "    vocab = sorted(list(vocab_chars))\n",
    "    print(f\"\\n3Ô∏è‚É£  Initial character vocabulary: {len(vocab)} unique characters\")\n",
    "    print(f\"   Characters: {vocab[:30]}{'...' if len(vocab) > 30 else ''}\")\n",
    "    \n",
    "    # Step 4: Represent words as character sequences\n",
    "    word_tokens = [list(word) for word in words]\n",
    "    print(f\"\\n4Ô∏è‚É£  Converted to character sequences\")\n",
    "    print(f\"   Example: '{words[0]}' ‚Üí {word_tokens[0]}\")\n",
    "    \n",
    "    return word_tokens, vocab, words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440cab0a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 6: BPE HELPER FUNCTIONS\n",
    "\n",
    "Core BPE Algorithm Functions\n",
    "\n",
    "These functions implement the two key operations:\n",
    "1. Count pair frequencies across all words\n",
    "2. Merge a specific pair in all words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c81dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_stats(word_tokens):\n",
    "    \"\"\"\n",
    "    Count frequencies of all adjacent character pairs.\n",
    "    \n",
    "    Args:\n",
    "        word_tokens (list): Words as lists of characters\n",
    "        \n",
    "    Returns:\n",
    "        Counter: Pair frequencies {(char1, char2): count}\n",
    "        \n",
    "    Example:\n",
    "        [['t','h','e','</w>'], ['t','h','e','</w>']]\n",
    "        ‚Üí {('t','h'): 2, ('h','e'): 2, ('e','</w>'): 2}\n",
    "    \"\"\"\n",
    "    pairs = Counter()\n",
    "    for word in word_tokens:\n",
    "        for i in range(len(word) - 1):\n",
    "            pair = (word[i], word[i+1])\n",
    "            pairs[pair] += 1\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb0cf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(word_tokens, pair_to_merge):\n",
    "    \"\"\"\n",
    "    Merge a specific pair in all words.\n",
    "    \n",
    "    Args:\n",
    "        word_tokens (list): Words as character sequences\n",
    "        pair_to_merge (tuple): (char1, char2) to merge\n",
    "        \n",
    "    Returns:\n",
    "        list: Updated word_tokens with pair merged\n",
    "        \n",
    "    Example:\n",
    "        word = ['t','h','e','</w>'], pair = ('t','h')\n",
    "        ‚Üí ['th','e','</w>']\n",
    "    \"\"\"\n",
    "    a, b = pair_to_merge\n",
    "    new_token = a + b\n",
    "    \n",
    "    new_word_tokens = []\n",
    "    for word in word_tokens:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            # Check if current position matches the pair\n",
    "            if i < len(word) - 1 and word[i] == a and word[i+1] == b:\n",
    "                new_word.append(new_token)\n",
    "                i += 2  # Skip both characters\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word_tokens.append(new_word)\n",
    "    \n",
    "    return new_word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbaf2da",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 7: TRAIN BPE VOCABULARY\n",
    "\n",
    "BPE Training Algorithm\n",
    "\n",
    "This is the heart of the tokenizer. The algorithm:\n",
    "1. Starts with a character-level vocabulary\n",
    "2. Iteratively finds the most frequent character pair\n",
    "3. Merges that pair into a new token\n",
    "4. Repeats until reaching the target vocabulary size\n",
    "\n",
    "The order of merges is CRITICAL - we save it for later use during tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b28d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(word_tokens, vocab, vocab_size=4000, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a BPE vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        word_tokens (list): Words as character sequences\n",
    "        vocab (list): Initial vocabulary (characters)\n",
    "        vocab_size (int): Target vocabulary size\n",
    "        verbose (bool): Print progress\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (final_vocab, merges, word_tokens)\n",
    "            - final_vocab: Complete vocabulary including merged tokens\n",
    "            - merges: Ordered list of merge operations\n",
    "            - word_tokens: Final word representations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING BPE VOCABULARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    print(f\"\\nTarget vocab size: {vocab_size}\")\n",
    "    print(f\"Initial vocab size: {len(vocab)}\")\n",
    "    print(f\"Merges needed: {num_merges}\\n\")\n",
    "    \n",
    "    # Store merge history (CRUCIAL for tokenization)\n",
    "    merges = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Main BPE loop\n",
    "    for i in tqdm(range(num_merges), desc=\"Training BPE\", disable=not verbose):\n",
    "        # Count pair frequencies\n",
    "        pairs = get_pair_stats(word_tokens)\n",
    "        \n",
    "        if not pairs:\n",
    "            print(f\"\\n‚ö†Ô∏è  No more pairs to merge at iteration {i}\")\n",
    "            break\n",
    "        \n",
    "        # Find most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        best_count = pairs[best_pair]\n",
    "        \n",
    "        # Save merge operation\n",
    "        merges.append(best_pair)\n",
    "        \n",
    "        # Merge in all words\n",
    "        word_tokens = merge_pair(word_tokens, best_pair)\n",
    "        \n",
    "        # Add new token to vocabulary\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        vocab.append(new_token)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ BPE TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Final vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Total merges performed: {len(merges)}\")\n",
    "    print(f\"Training time: {int(elapsed//60)}m {elapsed%60:.1f}s\")\n",
    "    \n",
    "    # Show examples of learned tokens\n",
    "    print(f\"\\nüìù Sample learned tokens (last 20):\")\n",
    "    for i, token in enumerate(vocab[-20:], 1):\n",
    "        print(f\"   {len(vocab)-20+i:4d}. '{token}'\")\n",
    "    \n",
    "    return vocab, merges, word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb23a43",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 8: SAVE AND LOAD VOCABULARY\n",
    "\n",
    "Persistence Functions\n",
    "\n",
    "Save the trained vocabulary and merge operations to disk.\n",
    "This allows you to:\n",
    "1. Reuse the tokenizer without retraining\n",
    "2. Share the tokenizer with others\n",
    "3. Version control your tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c4b345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_and_merges(vocab, merges, dataset_name, output_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Save vocabulary and merges to files.\n",
    "    \n",
    "    Args:\n",
    "        vocab (list): Complete vocabulary\n",
    "        merges (list): List of merge operations\n",
    "        dataset_name (str): Name for output files\n",
    "        output_dir (str): Directory to save files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_path = f\"{output_dir}/{dataset_name}_vocab.txt\"\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        for token in vocab:\n",
    "            f.write(f\"{token}\\n\")\n",
    "    \n",
    "    # Save merges (CRITICAL - preserves merge order)\n",
    "    merges_path = f\"{output_dir}/{dataset_name}_merges.txt\"\n",
    "    with open(merges_path, 'w', encoding='utf-8') as f:\n",
    "        for a, b in merges:\n",
    "            f.write(f\"{a} {b}\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved vocabulary to: {vocab_path}\")\n",
    "    print(f\"‚úÖ Saved merges to: {merges_path}\")\n",
    "    print(f\"   Vocab size: {len(vocab)}\")\n",
    "    print(f\"   Merge operations: {len(merges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65098b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_and_merges(dataset_name, data_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Load vocabulary and merges from files.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of saved files\n",
    "        data_dir (str): Directory containing files\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vocab, merges)\n",
    "    \"\"\"\n",
    "    vocab_path = f\"{data_dir}/{dataset_name}_vocab.txt\"\n",
    "    merges_path = f\"{data_dir}/{dataset_name}_merges.txt\"\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        vocab = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    \n",
    "    # Load merges\n",
    "    with open(merges_path, 'r', encoding='utf-8') as f:\n",
    "        merges = [tuple(line.strip().split()) for line in f.readlines()]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded vocabulary from: {vocab_path}\")\n",
    "    print(f\"‚úÖ Loaded merges from: {merges_path}\")\n",
    "    print(f\"   Vocab size: {len(vocab)}\")\n",
    "    print(f\"   Merge operations: {len(merges)}\")\n",
    "    \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eeb277",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 9: TOKENIZER CLASS\n",
    "\n",
    "Tokenizer Class\n",
    "\n",
    "This class provides a clean interface for encoding and decoding text.\n",
    "\n",
    "Key methods:\n",
    "- tokenize(text): Convert text to token IDs\n",
    "- decode(token_ids): Convert token IDs back to text\n",
    "\n",
    "The tokenizer applies the same merge operations in the same order as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d5ed2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    BPE Tokenizer for encoding and decoding text with pretokenization.\n",
    "    \n",
    "    Attributes:\n",
    "        vocab (list): Complete vocabulary\n",
    "        token_to_id (dict): Maps tokens to IDs\n",
    "        id_to_token (dict): Maps IDs to tokens\n",
    "        merges (list): Ordered merge operations\n",
    "        \n",
    "    Example:\n",
    "        >>> tokenizer = Tokenizer(vocab, merges)\n",
    "        >>> ids = tokenizer.tokenize(\"hello world\")\n",
    "        >>> text = tokenizer.decode(ids)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, merges):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer with vocabulary and merges.\n",
    "        \n",
    "        Args:\n",
    "            vocab (list): List of tokens\n",
    "            merges (list): List of (char1, char2) tuples in merge order\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
    "        self.id_to_token = {i: tok for tok, i in self.token_to_id.items()}\n",
    "        self.merges = merges\n",
    "        \n",
    "        print(f\"‚úÖ Tokenizer initialized\")\n",
    "        print(f\"   Vocabulary size: {len(vocab)}\")\n",
    "        print(f\"   Merge rules: {len(merges)}\")\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Encode text to token IDs using pretokenization + BPE algorithm.\n",
    "        \n",
    "        The algorithm:\n",
    "        1. PRETOKENIZE: Split into words and punctuation\n",
    "        2. Add </w> markers\n",
    "        3. Start with character-level representation\n",
    "        4. Apply merge operations in training order\n",
    "        5. Convert tokens to IDs\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            list: Token IDs\n",
    "            \n",
    "        Example:\n",
    "            >>> tokenizer.tokenize(\"Hello, world!\")\n",
    "            [1234, 45, 5678, 90, ...]  # IDs for ['Hello</w>', ',</w>', 'world</w>', '!</w>']\n",
    "        \"\"\"\n",
    "        # Step 1: PRETOKENIZE (same as training)\n",
    "        words = pretokenize(text)\n",
    "        \n",
    "        token_ids = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Add end-of-word marker\n",
    "            word = word + '</w>'\n",
    "            \n",
    "            # Start with character-level tokens\n",
    "            word_tokens = list(word)\n",
    "            \n",
    "            # Apply merges in order\n",
    "            for merge_pair in self.merges:\n",
    "                a, b = merge_pair\n",
    "                new_token = a + b\n",
    "                \n",
    "                i = 0\n",
    "                new_word_tokens = []\n",
    "                \n",
    "                while i < len(word_tokens):\n",
    "                    # Check if current and next character match the pair\n",
    "                    if (i < len(word_tokens) - 1 and \n",
    "                        word_tokens[i] == a and \n",
    "                        word_tokens[i+1] == b):\n",
    "                        new_word_tokens.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word_tokens.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                \n",
    "                word_tokens = new_word_tokens\n",
    "            \n",
    "            # Convert tokens to IDs\n",
    "            for token in word_tokens:\n",
    "                if token in self.token_to_id:\n",
    "                    token_ids.append(self.token_to_id[token])\n",
    "                else:\n",
    "                    # Handle unknown tokens by splitting to characters\n",
    "                    for char in token:\n",
    "                        if char in self.token_to_id:\n",
    "                            token_ids.append(self.token_to_id[char])\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode token IDs back to text.\n",
    "        \n",
    "        Args:\n",
    "            token_ids (list): List of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            str: Decoded text\n",
    "            \n",
    "        Example:\n",
    "            >>> tokenizer.decode([1234, 5678, 90])\n",
    "            \"hello, world!\"\n",
    "        \"\"\"\n",
    "        # Convert IDs to tokens\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.id_to_token:\n",
    "                tokens.append(self.id_to_token[token_id])\n",
    "        \n",
    "        # Concatenate tokens\n",
    "        text = ''.join(tokens)\n",
    "        \n",
    "        # Replace end-of-word markers with spaces\n",
    "        text = text.replace('</w>', ' ')\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tokenizer(vocab_size={len(self.vocab)}, merges={len(self.merges)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4054442",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 10: EXAMPLE USAGE AND TESTING\n",
    "\n",
    "Example Usage\n",
    "\n",
    "This section demonstrates how to use the tokenizer once trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c38caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizer(tokenizer, test_phrases):\n",
    "    \"\"\"\n",
    "    Test tokenizer with sample phrases.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (Tokenizer): Trained tokenizer\n",
    "        test_phrases (list): List of test strings\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING TOKENIZER\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for phrase in test_phrases:\n",
    "        print(f\"\\nüìù Input: \\\"{phrase}\\\"\")\n",
    "        \n",
    "        # Encode\n",
    "        token_ids = tokenizer.tokenize(phrase)\n",
    "        print(f\"   Token IDs: {token_ids}\")\n",
    "        \n",
    "        # Show actual tokens\n",
    "        tokens = [tokenizer.id_to_token[id] for id in token_ids]\n",
    "        print(f\"   Tokens: {tokens}\")\n",
    "        \n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(token_ids)\n",
    "        print(f\"   Decoded: \\\"{decoded}\\\"\")\n",
    "        \n",
    "        # Verify\n",
    "        if phrase.lower() == decoded:\n",
    "            print(\"   ‚úÖ Perfect reconstruction!\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Mismatch (expected: \\\"{phrase.lower()}\\\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7132163",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### SECTION 11: MAIN EXECUTION SCRIPT\n",
    "\n",
    "Main Script\n",
    "\n",
    "This is the complete pipeline to train and test a BPE tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea45bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_new=True, dataset_key=\"stories\", vocab_size=2000, data_fraction=1.0):\n",
    "    \"\"\"\n",
    "    Complete BPE tokenizer training pipeline with pretokenization.\n",
    "    \n",
    "    Args:\n",
    "        train_new (bool): If True, train new tokenizer. If False, load existing.\n",
    "        dataset_key (str): Key from DATASETS dictionary\n",
    "        vocab_size (int): Target vocabulary size (only used if train_new=True)\n",
    "        data_fraction (float): Fraction of data to use (only used if train_new=True)\n",
    "    \n",
    "    Returns:\n",
    "        Tokenizer: Trained or loaded tokenizer\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"BPE TOKENIZER TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get dataset name\n",
    "    dataset_name = DATASETS[dataset_key][\"dataset_name\"]\n",
    "    \n",
    "    if not train_new:\n",
    "        # ==================================================================\n",
    "        # LOADING EXISTING TOKENIZER\n",
    "        # ==================================================================\n",
    "        print(\"\\nüîÑ Loading existing tokenizer...\")\n",
    "        print(f\"   Dataset: {dataset_name}\")\n",
    "        \n",
    "        try:\n",
    "            vocab, merges = load_vocab_and_merges(dataset_name)\n",
    "            tokenizer = Tokenizer(vocab, merges)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"‚úÖ TOKENIZER LOADED SUCCESSFULLY!\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            return tokenizer\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"\\n‚ùå Error: Could not find saved tokenizer files!\")\n",
    "            print(f\"   {e}\")\n",
    "            print(f\"\\nüí° Tip: Set train_new=True to train a new tokenizer\")\n",
    "            return None\n",
    "    \n",
    "    else:\n",
    "        # ==================================================================\n",
    "        # TRAINING NEW TOKENIZER\n",
    "        # ==================================================================\n",
    "        print(f\"\\nüöÄ Training new tokenizer...\")\n",
    "        print(f\"   Dataset: {dataset_key}\")\n",
    "        print(f\"   Vocab size: {vocab_size}\")\n",
    "        print(f\"   Data fraction: {data_fraction*100:.0f}%\")\n",
    "        \n",
    "        # Step 1: Load dataset\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 1: LOADING DATASET\")\n",
    "        print(\"=\"*80)\n",
    "        dataset, dataset_name, text_column = load_text_dataset(dataset_key)\n",
    "        \n",
    "        # Step 2: Save to file\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 2: SAVING DATASET TO FILE\")\n",
    "        print(\"=\"*80)\n",
    "        text, output_path = save_dataset_to_file(\n",
    "            dataset, dataset_name, text_column, \n",
    "            fraction=data_fraction\n",
    "        )\n",
    "        \n",
    "        # Step 3: Prepare for BPE (with pretokenization)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 3: PREPARING TEXT (WITH PRETOKENIZATION)\")\n",
    "        print(\"=\"*80)\n",
    "        word_tokens, vocab, words = prepare_text_for_bpe(text)\n",
    "        \n",
    "        # Step 4: Train BPE\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 4: TRAINING BPE\")\n",
    "        print(\"=\"*80)\n",
    "        final_vocab, merges, final_word_tokens = train_bpe(\n",
    "            word_tokens, vocab, \n",
    "            vocab_size=vocab_size\n",
    "        )\n",
    "        \n",
    "        # Step 5: Save vocabulary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 5: SAVING VOCABULARY\")\n",
    "        print(\"=\"*80)\n",
    "        save_vocab_and_merges(final_vocab, merges, dataset_name)\n",
    "        \n",
    "        # Step 6: Create tokenizer\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 6: CREATING TOKENIZER\")\n",
    "        print(\"=\"*80)\n",
    "        tokenizer = Tokenizer(final_vocab, merges)\n",
    "        \n",
    "        # Step 7: Test tokenizer (with punctuation)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 7: TESTING TOKENIZER\")\n",
    "        print(\"=\"*80)\n",
    "        test_phrases = [\n",
    "            \"Harry Potter and the Philosopher's Stone.\",\n",
    "            \"Once upon a time, there was a wizard.\",\n",
    "            \"The magical world of Hogwarts!\",\n",
    "            \"Hermione said: 'Don't worry!'\"\n",
    "        ]\n",
    "        test_tokenizer(tokenizer, test_phrases)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ PIPELINE COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nYour tokenizer is ready to use!\")\n",
    "        print(f\"Vocabulary saved to: ./data/{dataset_name}_vocab.txt\")\n",
    "        print(f\"Merges saved to: ./data/{dataset_name}_merges.txt\")\n",
    "        \n",
    "        return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c192c4",
   "metadata": {},
   "source": [
    "### SECTION 12: SCRIPT ENTRY POINT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9aeca2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ MODE: Loading existing tokenizer\n",
      "================================================================================\n",
      "BPE TOKENIZER TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "üîÑ Loading existing tokenizer...\n",
      "   Dataset: AI_Storyteller_Dataset\n",
      "\n",
      "‚úÖ Loaded vocabulary from: ./data/AI_Storyteller_Dataset_vocab.txt\n",
      "‚úÖ Loaded merges from: ./data/AI_Storyteller_Dataset_merges.txt\n",
      "   Vocab size: 3000\n",
      "   Merge operations: 2950\n",
      "‚úÖ Tokenizer initialized\n",
      "   Vocabulary size: 3000\n",
      "   Merge rules: 2950\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TOKENIZER LOADED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE: USING THE TRAINED TOKENIZER\n",
      "================================================================================\n",
      "\n",
      "üìù Input: \"once upon a time in a land far away\"\n",
      "   Token IDs: [724, 970, 77, 352, 97, 77, 592, 1826, 691]\n",
      "   Num tokens: 9\n",
      "   Tokens: ['once</w>', 'upon</w>', 'a</w>', 'time</w>', 'in</w>', 'a</w>', 'land</w>', 'far</w>', 'away</w>']\n",
      "   Decoded: \"once upon a time in a land far away\"\n",
      "   Compression: 35 chars ‚Üí 9 tokens (3.89x)\n",
      "\n",
      "üìù Input: \"he was a brave knight\"\n",
      "   Token IDs: [147, 92, 77, 995, 137, 29, 528]\n",
      "   Num tokens: 7\n",
      "   Tokens: ['he</w>', 'was</w>', 'a</w>', 'bra', 've</w>', 'k', 'night</w>']\n",
      "   Decoded: \"he was a brave knight\"\n",
      "   Compression: 21 chars ‚Üí 7 tokens (3.00x)\n",
      "\n",
      "üìù Input: \"it's a beautiful day, isn't it?\"\n",
      "   Token IDs: [2862, 77, 1991, 85, 430, 355, 63, 462, 227, 127, 992]\n",
      "   Num tokens: 11\n",
      "   Tokens: [\"it's</w>\", 'a</w>', 'beau', 'ti', 'ful</w>', 'day</w>', ',</w>', 'is', \"n't</w>\", 'it</w>', '?</w>']\n",
      "   Decoded: \"it's a beautiful day , isn't it ?\"\n",
      "   Compression: 31 chars ‚Üí 11 tokens (2.82x)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ======================================================================\n",
    "    # CONFIGURATION\n",
    "    # ======================================================================\n",
    "    \n",
    "    # Set to True to train a new tokenizer, False to load existing\n",
    "    TRAIN_NEW = False\n",
    "    \n",
    "    # Dataset selection (if training new)\n",
    "    DATASET_KEY = \"stories\"  # Options: \"harry_potter\", \"stories\"\n",
    "    \n",
    "    # Vocabulary size (if training new)\n",
    "    VOCAB_SIZE = 3000\n",
    "    \n",
    "    # Data fraction (if training new)\n",
    "    DATA_FRACTION = 1.0  # Use 100% of data\n",
    "    \n",
    "    # ======================================================================\n",
    "    # RUN PIPELINE\n",
    "    # ======================================================================\n",
    "    \n",
    "    if TRAIN_NEW:\n",
    "        print(\"üöÄ MODE: Training new tokenizer\")\n",
    "        tokenizer = main(\n",
    "            train_new=True,\n",
    "            dataset_key=DATASET_KEY,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            data_fraction=DATA_FRACTION\n",
    "        )\n",
    "    else:\n",
    "        print(\"üîÑ MODE: Loading existing tokenizer\")\n",
    "        tokenizer = main(\n",
    "            train_new=False,\n",
    "            dataset_key=DATASET_KEY\n",
    "        )\n",
    "    \n",
    "    # ======================================================================\n",
    "    # EXAMPLE USAGE\n",
    "    # ======================================================================\n",
    "    \n",
    "    if tokenizer is not None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXAMPLE: USING THE TRAINED TOKENIZER\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        sample_texts = [\n",
    "            \"once upon a time in a land far away\",\n",
    "            \"he was a brave knight\",\n",
    "            \"it's a beautiful day, isn't it?\"\n",
    "        ]\n",
    "        \n",
    "        for sample_text in sample_texts:\n",
    "            print(f\"\\nüìù Input: \\\"{sample_text}\\\"\")\n",
    "            \n",
    "            # Encode\n",
    "            ids = tokenizer.tokenize(sample_text)\n",
    "            print(f\"   Token IDs: {ids}\")\n",
    "            print(f\"   Num tokens: {len(ids)}\")\n",
    "            \n",
    "            # Show tokens\n",
    "            tokens = [tokenizer.id_to_token[id] for id in ids]\n",
    "            print(f\"   Tokens: {tokens}\")\n",
    "            \n",
    "            # Decode\n",
    "            reconstructed = tokenizer.decode(ids)\n",
    "            print(f\"   Decoded: \\\"{reconstructed}\\\"\")\n",
    "            \n",
    "            # Compression ratio\n",
    "            orig_chars = len(sample_text)\n",
    "            token_count = len(ids)\n",
    "            ratio = orig_chars / token_count if token_count > 0 else 0\n",
    "            print(f\"   Compression: {orig_chars} chars ‚Üí {token_count} tokens ({ratio:.2f}x)\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
